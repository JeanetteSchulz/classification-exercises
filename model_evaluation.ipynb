{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c84dd87",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "<hr style=\"border:2px solid red\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971f1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# import splitting and imputing functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# turn off pink boxes for demo\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import our own acquire module\n",
    "import acquire\n",
    "\n",
    "# Remove limits on viewing dataframes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec73683",
   "metadata": {},
   "source": [
    "### 2. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "\n",
    "- In the context of this problem, what is a false positive?\n",
    "    - Assuming positive is a cat and negative is not a cat (dog)\n",
    "    - False Positive: The photo is of a dog, but the prediction is a cat\n",
    "- In the context of this problem, what is a false negative?\n",
    "    - The photo is actually of a cat, but it is predicted to be a dog \n",
    "- How would you describe this model?\n",
    "    - complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c325417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of these evaluation metrics are as follows:\n",
      "Accuracy: 0.8\n",
      "Recall: 0.72\n",
      "Precision: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Based on the confusion matrix above, I can put the numbers into thier respective outcomes:\n",
    "true_positive = 34\n",
    "true_negative = 46\n",
    "false_positive = 7\n",
    "false_negative = 13\n",
    "\n",
    "# Now to use the formulas given in the curriculum for my evaluation metrics:\n",
    "accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "\n",
    "# Making a pretty print statement so it's easier to read:\n",
    "print(\"The accuracy of these evaluation metrics are as follows:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", round(recall,2))\n",
    "print(\"Precision:\", round(precision,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070a829",
   "metadata": {},
   "source": [
    "### 3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "### Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found in \"c3.csv\".\n",
    "\n",
    "### Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "#### An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d85a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aquiring the dataset:\n",
    "ducks = pd.read_csv(\"c3.csv\")\n",
    "ducks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01880350",
   "metadata": {},
   "source": [
    "### The problem states: \"...they want to identify as many of the ducks that have a defect as possible.\" \n",
    "\n",
    "#### Thinking through the problem...\n",
    "- What is the positive and negative case?\n",
    "    - When determining Positive/Negative, make life easier by making a correct identification as Positive \n",
    "    - The way I think of this is: if you were given a list and asked to find the lines that said it was a defective duck, each time you found one would be a Positive Identification!\n",
    "   \n",
    "   - Positive: A duck is identified to be defective\n",
    "    - Negative: A duck is not identified as defective \n",
    "    \n",
    "- What are the possible outcomes?\n",
    "    - True Positive: A duck is defective and it does not get sold\n",
    "    - True Negative: A duck is not defective and gets sold\n",
    "    - False Positive: A duck is not defective, but is marked as defective, and does not get sold\n",
    "    - False Negative: A duck is defective, but is not marked as defective, and ends up getting sold\n",
    "\n",
    "### Which evaluation metric would be appropriate here? \n",
    "- Codeup Cody Creator wants to over identify than under identify. With this in mind, I think Recall would be best because a False Negative is more costly than a False Positive. \n",
    "\n",
    "### Which model would be the best fit for this use case?¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fcc58b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    184\n",
       "Defect        16\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alright, time for some legwork, or in this case, a lot of code!\n",
    "# Which label (actual) appears most frequently in my dataset?\n",
    "ducks.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353b7a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codeup Cody Creator Model Accuracies\n",
      "=====================================\n",
      "baseline accuracy: 92.00%\n",
      "model one accuracy: 95.00%\n",
      "model two accuracy: 56.00%\n",
      "model three accuracy: 55.50%\n"
     ]
    }
   ],
   "source": [
    "# Model and baseline accuracy:\n",
    "# First I'll create a new column called 'baseline_prediction'\n",
    "# which will be given the most frequent label from actual (in this case 'No Defect')\n",
    "# this baseline is in no way related to the baselines used int the evaluation matrixes \n",
    "ducks['baseline_prediction'] = 'No Defect'\n",
    "\n",
    "# The data already has 3 columns dedicated to model predictions, so I'll check all three for accuracy:\n",
    "model1_accuracy = (ducks.actual == ducks.model1).mean()\n",
    "model2_accuracy = (ducks.actual == ducks.model2).mean()\n",
    "model3_accuracy = (ducks.actual == ducks.model3).mean()\n",
    "\n",
    "# And get a base line accuracy for comparison:\n",
    "baseline_accuracy = (ducks.actual == ducks.baseline_prediction).mean()\n",
    "\n",
    "print(\"Codeup Cody Creator Model Accuracies\")\n",
    "print(\"=====================================\")\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')\n",
    "print(f'model one accuracy: {model1_accuracy:.2%}')\n",
    "print(f'model two accuracy: {model2_accuracy:.2%}')\n",
    "print(f'model three accuracy: {model3_accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0058ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>baseline_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual     model1     model2  model3 baseline_prediction\n",
       "13  Defect  No Defect     Defect  Defect           No Defect\n",
       "30  Defect     Defect  No Defect  Defect           No Defect\n",
       "65  Defect     Defect     Defect  Defect           No Defect\n",
       "70  Defect     Defect     Defect  Defect           No Defect\n",
       "74  Defect  No Defect  No Defect  Defect           No Defect"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall Evaluation\n",
    "# Recall is the percentage of positive cases that a model accurately predicted\n",
    "\n",
    "subset = ducks[ducks.actual == 'Defect']\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bf470f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 recall: 50 %\n",
      "model2 recall: 56 %\n",
      "model3 recall: 81 %\n"
     ]
    }
   ],
   "source": [
    "# I like making loops; so let's make a loop that finds the recall of all our models\n",
    "models = [\"model1\" , \"model2\" , \"model3\"]\n",
    "\n",
    "for x in models: \n",
    "    model_recall = ( subset.actual == subset[ x ] ).mean()\n",
    "    print(x, \"recall:\", round(model_recall * 100),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52aefd",
   "metadata": {},
   "source": [
    "> ANSWER \"Which model would be the best fit for this use case\":\n",
    ">\n",
    "> <b>Model 3</b> would be the best to use for a Recall Evaluation, as it gives us the best accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83514855",
   "metadata": {},
   "source": [
    "### Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. \n",
    "\n",
    "\n",
    "### Which evaluation metric would be appropriate here? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd466c9",
   "metadata": {},
   "source": [
    "- The company would rather a defect get sold, than a vacation go to someone with a non-defective duck.\n",
    "- So for this case, a False Positive is more costly than a False Negative. Therefore, I believe a Precision evaluation would be best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75e356",
   "metadata": {},
   "source": [
    "### Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b113a2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision: 80 %\n",
      "model2 precision: 10 %\n",
      "model3 precision: 13 %\n"
     ]
    }
   ],
   "source": [
    "# Precision Evaluation\n",
    "# Precision is the percentage of positive predictions that the model made, that are correct.\n",
    "# (i.e. model prediction == 'Defect')\n",
    "\n",
    "# Loopy Time ~\n",
    "for x in models:\n",
    "    # choose subset of model1 where we only select 'positive predictions'\n",
    "    subset = ducks[ducks[ x ] == 'Defect']\n",
    "\n",
    "    # calculate precision\n",
    "    model_precision = ( subset.actual == subset[ x ] ).mean()\n",
    "    \n",
    "    print(x, \"precision:\", round(model_precision*100), \"%\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac7174",
   "metadata": {},
   "source": [
    "> ANSWER \"Which model would be the best fit for this use case\": \n",
    ">\n",
    "><b>Model 1</b> would be the best to use for a Precision Evaluation, as it will minimize the False Positive predictions of defects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "650862a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x in models:\\n    model_subset = ducks[ducks[ x ] == 'Defect']\\n    model_precision = (model_subset.model1 == model_subset.actual).mean()\\n\\n    baseline_subset = ducks[ducks.baseline_prediction == 'Defect']\\n    baseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\\n\\n    print(f'model precision: {model_precision:.2%}')\\n    print(f'baseline precision: {baseline_precision:.2%}')\\n    \\n    # Model Two\\nmodel_subset = ducks[ducks.model2 == 'No Defect']\\nmodel_precision = (model_subset.model2 == model_subset.actual).mean()\\n\\nbaseline_subset = ducks[ducks.baseline_prediction == 'No Defect']\\nbaseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\\n\\nprint(f'model precision: {model_precision:.2%}')\\nprint(f'baseline precision: {baseline_precision:.2%}')\\n\\n# Model Three\\nmodel_subset = ducks[ducks.model3 == 'No Defect']\\nmodel_precision = (model_subset.model3 == model_subset.actual).mean()\\n\\nbaseline_subset = ducks[ducks.baseline_prediction == 'No Defect']\\nbaseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\\n\\nprint(f'model precision: {model_precision:.2%}')\\nprint(f'baseline precision: {baseline_precision:.2%}')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scratch work\n",
    "# I'm not certain why my answer is different than the instructor/ where i was going with this\n",
    "# So I'll leave this here to review at a later time\n",
    "\"\"\"\n",
    "for x in models:\n",
    "    model_subset = ducks[ducks[ x ] == 'Defect']\n",
    "    model_precision = (model_subset.model1 == model_subset.actual).mean()\n",
    "\n",
    "    baseline_subset = ducks[ducks.baseline_prediction == 'Defect']\n",
    "    baseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\n",
    "\n",
    "    print(f'model precision: {model_precision:.2%}')\n",
    "    print(f'baseline precision: {baseline_precision:.2%}')\n",
    "    \n",
    "    # Model Two\n",
    "model_subset = ducks[ducks.model2 == 'No Defect']\n",
    "model_precision = (model_subset.model2 == model_subset.actual).mean()\n",
    "\n",
    "baseline_subset = ducks[ducks.baseline_prediction == 'No Defect']\n",
    "baseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')\n",
    "\n",
    "# Model Three\n",
    "model_subset = ducks[ducks.model3 == 'No Defect']\n",
    "model_precision = (model_subset.model3 == model_subset.actual).mean()\n",
    "\n",
    "baseline_subset = ducks[ducks.baseline_prediction == 'No Defect']\n",
    "baseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3aebc",
   "metadata": {},
   "source": [
    "### 4. You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "### At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "### Several models have already been developed with the data, and you can find their results in the \"gives_you_paws.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c50efbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aquiring the dataset:\n",
    "paws = pd.read_csv(\"gives_you_paws.csv\")\n",
    "paws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ed89ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    3254\n",
       "cat    1746\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking value counts\n",
    "paws.actual.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a569f1",
   "metadata": {},
   "source": [
    "### Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n",
    "\n",
    "### a. In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f785d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_prediction accuracy: 65.08%\n",
      "model1 accuracy: 80.74%\n",
      "model2 accuracy: 63.04%\n",
      "model3 accuracy: 50.96%\n",
      "model4 accuracy: 74.26%\n"
     ]
    }
   ],
   "source": [
    "# First, I'll create baseline using the highest value count, which is \"dog\"\n",
    "paws['baseline_prediction'] = 'dog'\n",
    "paws.head()\n",
    "\n",
    "models = ['baseline_prediction', 'model1', 'model2', 'model3', 'model4' ]\n",
    "\n",
    "for x in models:\n",
    "    model_accuracy = ( paws.actual == paws[ x ] ).mean()\n",
    "    print(f'{x} accuracy: {model_accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4becf990",
   "metadata": {},
   "source": [
    "> Looks like <b>Model 1</b> and <b>Model 4</b> are better than the baseline in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7b177e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model1', 'model2', 'model3', 'model4', 'baseline_prediction']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instructor Solution\n",
    "paws[\"baseline_prediction\"] = paws.actual.value_counts().idxmax()\n",
    "\n",
    "# Calling columns to make a list instead of writing them\n",
    "models = list(paws.columns)\n",
    "models = models[1:]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585275c5",
   "metadata": {},
   "source": [
    "### b. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9783dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 recall: 80 %\n",
      "model2 recall: 49 %\n",
      "model3 recall: 51 %\n",
      "model4 recall: 96 %\n",
      "baseline_prediction recall: 100 %\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: An automated algorithm tags pictures as either a cat or a dog\n",
    "# For Phase 1, I should choose a model with highest Recall\n",
    "\n",
    "# Recall Evaluation\n",
    "# Recall is the percentage of positive cases that a model accurately predicted\n",
    "# For this case, since they solely deal with dogs, I'll make 'dog' the Positive Identification  \n",
    "subset = paws[paws.actual == 'dog']\n",
    "\n",
    "# I'm gonna resuse my loop that finds the recall of all the models \n",
    "for x in models: \n",
    "    model_recall = ( subset.actual == subset[ x ] ).mean()\n",
    "    print(x, \"recall:\", round(model_recall * 100),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6a6e2",
   "metadata": {},
   "source": [
    "> For Phase One, it looks like <b>Model 4</b> would be the best for Recall as it will minimize the False Negative predictions of dogs. \n",
    ">\n",
    ">i.e. it will minimize tagging a picture of a dog as not a dog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385c163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 precision: 89.0 %\n",
      "model2 precision: 89.32 %\n",
      "model3 precision: 65.99 %\n",
      "model4 precision: 73.12 %\n",
      "baseline_prediction precision: 65.08 %\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Represents photos that have been initially identified, and are put through another round tagging\n",
    "# I will use Precision this time to minimize the False Positives, i.e. tagging a photo as a dog that is not a dog\n",
    "\n",
    "for x in models:\n",
    "    # choose subset of model1 where we only select 'positive predictions'\n",
    "    subset = paws[paws[ x ] == 'dog']\n",
    "\n",
    "    # calculate precision\n",
    "    model_precision = ( subset.actual == subset[ x ] ).mean()\n",
    "    \n",
    "    print(x, \"precision:\", round(model_precision*100, 2), \"%\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f100d",
   "metadata": {},
   "source": [
    "> For Phase Two, it looks like <b>Model 2</b> would be the best for Precision as it will minimize the False Positive predictions of dogs. \n",
    ">\n",
    ">i.e. it will minimize tagging a photo as a dog that is not a dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3f311",
   "metadata": {},
   "source": [
    "### c. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25f4e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase One Recall:\n",
      "model1 recall: 82 %\n",
      "model2 recall: 89 %\n",
      "model3 recall: 51 %\n",
      "model4 recall: 35 %\n",
      "\n",
      "Phase Two Precision:\n",
      "model1 precision: 68.98 %\n",
      "model2 precision: 48.41 %\n",
      "model3 precision: 35.83 %\n",
      "model4 precision: 80.72 %\n"
     ]
    }
   ],
   "source": [
    "# Time to copy/paste my dog stuff and change it to cats!\n",
    "\n",
    "# Phase 1: An automated algorithm tags pictures as either a cat or a dog\n",
    "# For Phase 1, I should choose a model with highest Recall\n",
    "\n",
    "print(\"Phase One Recall:\")\n",
    "# Recall Evaluation\n",
    "# Recall is the percentage of positive cases that a model accurately predicted\n",
    "# For this case, since they solely deal with cats, I'll make 'cat' the Positive Identification  \n",
    "models = ['model1', 'model2', 'model3', 'model4' ]\n",
    "subset = paws[paws.actual == 'cat']\n",
    "\n",
    "# I'm gonna resuse my loop that finds the recall of all the models \n",
    "for x in models: \n",
    "    model_recall = ( subset.actual == subset[ x ] ).mean()\n",
    "    print(x, \"recall:\", round(model_recall * 100),\"%\")\n",
    "\n",
    "    \n",
    "\n",
    "print(\"\\nPhase Two Precision:\")\n",
    "# Phase 2: Represents photos that have been initially identified, and are put through another round tagging\n",
    "# I will use Precision this time to minimize the False Positives, i.e. tagging a photo as a cat that is not a cat\n",
    "\n",
    "for x in models:\n",
    "    # choose subset of model1 where we only select 'positive predictions'\n",
    "    subset = paws[paws[ x ] == 'cat']\n",
    "\n",
    "    # calculate precision\n",
    "    model_precision = ( subset.actual == subset[ x ] ).mean()\n",
    "    \n",
    "    print(x, \"precision:\", round(model_precision*100,2), \"%\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010d51d",
   "metadata": {},
   "source": [
    "> For the Cat Team, Phase One, <b>Model 2</b> would be the best for Recall \n",
    "> \n",
    "> For the Cat Team, Phase Two, <b>Model 4</b> would be the best for Precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
