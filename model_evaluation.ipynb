{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6ba2aa",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "<hr style=\"border:2px solid red\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3ee31",
   "metadata": {},
   "source": [
    "### 2. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "\n",
    "- In the context of this problem, what is a false positive?\n",
    "    - Assuming positive is a cat and negative is a dog\n",
    "    - False Positive: The photo is of a dog, but the prediction is a cat\n",
    "- In the context of this problem, what is a false negative?\n",
    "- How would you describe this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a08ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# import splitting and imputing functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# turn off pink boxes for demo\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import our own acquire module\n",
    "import acquire\n",
    "\n",
    "# Remove limits on viewing dataframes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d5e46",
   "metadata": {},
   "source": [
    "### 3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "### Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found in \"c3.csv\".\n",
    "\n",
    "### Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "#### An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fd7678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual     model1     model2     model3\n",
       "0    No Defect  No Defect     Defect  No Defect\n",
       "1    No Defect  No Defect     Defect     Defect\n",
       "2    No Defect  No Defect     Defect  No Defect\n",
       "3    No Defect     Defect     Defect     Defect\n",
       "4    No Defect  No Defect     Defect  No Defect\n",
       "..         ...        ...        ...        ...\n",
       "195  No Defect  No Defect     Defect     Defect\n",
       "196     Defect     Defect  No Defect  No Defect\n",
       "197  No Defect  No Defect  No Defect  No Defect\n",
       "198  No Defect  No Defect     Defect     Defect\n",
       "199  No Defect  No Defect  No Defect     Defect\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ducks = pd.read_csv(\"c3.csv\")\n",
    "ducks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428a498",
   "metadata": {},
   "source": [
    "### They tell you that they want to identify as many of the ducks that have a defect as possible. \n",
    "\n",
    "#### Thinking through the problem...\n",
    "- What is the positive and negative case?\n",
    "    - Positive: A toy duck has no defects and makes it to the store\n",
    "    - Negative: A toy duck has a defect and does not make it to the store\n",
    "    \n",
    "- What are the possible outcomes?\n",
    "    - True Positive: A toy duck has no defects and makes it to the store\n",
    "    - True Negative: A toy duck has a defect and does not make it to the store\n",
    "    - False Positive: A toy duck has a defect and makes it to the store\n",
    "    - False Negative: A toy duck has no defects and does not make it to the store\n",
    "\n",
    "### Which evaluation metric would be appropriate here? \n",
    "- Codeup Cody Creator wants to over identify than under identify. With this in mind, I think Precision would be best because a False Positive is more costly than a False Negative. \n",
    "\n",
    "### Which model would be the best fit for this use case?¶\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f7cc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    184\n",
       "Defect        16\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alright, time for some legwork, or in this case, a lot of code!\n",
    "# Which label (actual) appears most frequently in my dataset?\n",
    "ducks.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04747db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 accuracy: 95.00%\n",
      "model2 accuracy: 56.00%\n",
      "model3 accuracy: 55.50%\n",
      "baseline accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# Model and baseline accuracy:\n",
    "# First I'll create a new column called 'baseline_prediction\n",
    "# which will be given the most frequent label from actual (in this case 'No Defect')\n",
    "ducks['baseline_prediction'] = 'No Defect'\n",
    "\n",
    "# The data already has 3 columns dedicated to model predictions, so I'll check all three for accuracy:\n",
    "model1_accuracy = (ducks.actual == ducks.model1).mean()\n",
    "model2_accuracy = (ducks.actual == ducks.model2).mean()\n",
    "model3_accuracy = (ducks.actual == ducks.model3).mean()\n",
    "\n",
    "# And get a base line accuracy for comparison:\n",
    "baseline_accuracy = (ducks.actual == ducks.baseline_prediction).mean()\n",
    "\n",
    "print(f'model1 accuracy: {model1_accuracy:.2%}')\n",
    "print(f'model2 accuracy: {model2_accuracy:.2%}')\n",
    "print(f'model3 accuracy: {model3_accuracy:.2%}')\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d83b645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision: 95.79%\n",
      "baseline precision: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# Precision: Where I only look at the subset of a model, where the model made positive predictions\n",
    "# (i.e. model prediction == 'No Defect')\n",
    "\n",
    "# Model One\n",
    "model_subset = ducks[ducks.model1 == 'No Defect']\n",
    "model_precision = (model_subset.model1 == model_subset.actual).mean()\n",
    "\n",
    "baseline_subset = ducks[ducks.baseline_prediction == 'No Defect']\n",
    "baseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1610e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision: 93.64%\n",
      "baseline precision: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# Model Two\n",
    "model_subset = ducks[ducks.model2 == 'No Defect']\n",
    "model_precision = (model_subset.model2 == model_subset.actual).mean()\n",
    "\n",
    "baseline_subset = ducks[ducks.baseline_prediction == 'No Defect']\n",
    "baseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fb9531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision: 97.03%\n",
      "baseline precision: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# Model Three\n",
    "model_subset = ducks[ducks.model3 == 'No Defect']\n",
    "model_precision = (model_subset.model3 == model_subset.actual).mean()\n",
    "\n",
    "baseline_subset = ducks[ducks.baseline_prediction == 'No Defect']\n",
    "baseline_precision = (baseline_subset.baseline_prediction == baseline_subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7e89a",
   "metadata": {},
   "source": [
    "#### Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565febb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
